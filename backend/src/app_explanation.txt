App Explanation

This is a simple backend for a RAG chatbot. It is currenty using the OpenAI API for embeddings and prompt completion. 
To explain how the app works, we will trace through an example API call.

First, read through api_handler.py.
This file defines two API endpoints - /get_query and /submit_query.
Let's say that the client makes a POST request to our /submit_query endpoint with the query "What is OpenVal"
The endpoint calls the function submit_query_endpoint using the query as type string and will return an object -- DBQueryModel.
In the function we instantiate a DBQueryModel object with request.query_text as our query_text. We save this object as new_query
Now, head to db_model.py. 
Read through this file.
Here, we describe what is essentially an [ORM.](https://www.prisma.io/dataguide/types/relational/what-is-an-orm#:~:text=An%20ORM%2C%20or%20Object%20Relational,used%20in%20object%2Doriented%20programming.)
Our DBQueryModel class encapulates the logic for interacting with the mysql database. In this class we have the put_item and
get_item methods to retrieve data from the database. The constructor of the class assigns a unique query_id, gives the query
a timestamp, sets the query_text to "What is OpenVal" and sets is_complete to False.

Back in api_handler.py we initiate the function query_rag(request.query_text). This sends our query to the file query_data.py
Read through this file. 
The function takes a string and will return a QueryResponse object. The first thing this function does is call get_chroma_db.py
Read through this file.
This sets up our local Chroma database. This is what we are using for our vector store for our embeddings. 
This file creates an instance of our vector database.

*Side Note* To be clear -- our Chroma database is already built. This is built when we run the script store_data.py.
This file loads and splits our pdfs of our data and then saves the chunks into our vector database after embedding them.

After calling get_chroma_db.py and returning our instance of the database the function query_rag runs a similary check to
return the top three chunks in Chroma db that are most similar to query_text.
We make sure that our comparison value is greater than .7 (1 being closest similarity 0 being dissimilar)
Then we take the 3 docs and scores from our results and join them into context_text.
We take our context_text and prompt_template and combine them into our prompt. 
We make a call to OpenAi using our prompt and save the response into response_text. 
*Side Note* I would like functionality for sources, or where the chunk we used for the answer to be returned to the user.
we dont have this working currently but some of the logic is there.

We return our QueryResponse object with our query_text, response_text, and eventually sources.

Back in api_handler.py in our submit_query_endpoint function we do the following:
save query_response.response_text to our new_query object new_query.answer_text
save query_response.sources to new_query.sources
set new_query.is_complete to True
finally, we call new_query.put_item
Go back to our ORM - db_model.py. The method put_item connects to the mysql server and insterts the DBQueryModel object.

So we have made a POST request, and returned our backend response to our database. 
Now, we can make GET request.
We make a call to our GET request and call get_query_endpoint.
This function takes a query_id and returns a DBQueryModel object.
We simply call DBQueryModel.get_item(query_id).
get_item takes the query, connects to the mysql database, selects all from database where query_id == query_id


